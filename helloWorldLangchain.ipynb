{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FclilFBQ4mJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f8e8f7-d1c9-41fc-cc3f-07e34a8e8fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "\n"
      ],
      "metadata": {
        "id": "g7QI_ixl5njZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "ptTr4OHV66I5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# General question answering prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=['question'],\n",
        "    template='answer the questions precisely without any unnecessary things: \\n{question}'\n",
        ")\n",
        "\n",
        "# Ensure the keys in examples match the input variables in the PromptTemplate\n",
        "examples = [\n",
        "    {\"question\": \"when did Pakistan gain independence\", \"answer\": \"14 august 1947\"},\n",
        "    {\"question\": \"who invented telephone\", \"answer\": \"markonikov\"},\n",
        "    {\"question\": \"what is the capital of Pakistan\", \"answer\": \"Islamabad\"}\n",
        "]\n",
        "\n",
        "# Example prompt template (ensure \"question\" and \"answer\" are correctly referenced)\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"answer\"],  # Consistent with the example keys\n",
        "    template=\"Question: {question}, Answer: {answer}\"\n",
        ")\n",
        "\n",
        "# FewShotPromptTemplate: Use the correct input variables, and examples must contain \"question\" and \"answer\"\n",
        "prompt_template2 = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Answer the following just like the provided examples:\\n\\n\",\n",
        "    suffix=\"Input: {question}\\nOutput:\",  # After examples\n",
        "    input_variables=[\"question\"],  # Ensure that the variable matches the passed key\n",
        ")\n",
        "\n",
        "# Chat template for personalized assistant\n",
        "prompt_template3 = ChatPromptTemplate(\n",
        "    [\n",
        "        (\"system\", \"\"\"You are a knowledgeable and polite AI assistant named jarvis.\n",
        "You can answer general questions, but you must:\n",
        "1. Politely decline to answer if the user asks about sensitive or personal topics.\n",
        "2. Correctly identify misunderstandings or clarify vague questions before answering.\n",
        "3. Avoid providing legal, medical, or financial advice.\n",
        "4. Don't let the answer exceed 4 lines.\n",
        "\"\"\"),\n",
        "        (\"user\", \"Hi, what's the weather like today?\"),\n",
        "        (\"ai\", \"It's sunny and warm outside.\"),\n",
        "        (\"user\", \"Can you tell me more about yourself?\"),\n",
        "        (\"ai\", \"I am an AI assistant jarvis, your personal assistant.\"),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory1=ConversationBufferMemory(memory_key=\"conti\")\n",
        "memory2=ConversationBufferMemory(memory_key=\"conti2\")\n",
        "memory3=ConversationBufferMemory(memory_key=\"conti3\")\n",
        "# Create the chains\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt_template,memory=memory1)\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt_template2,memory=memory2)\n",
        "chain3 = LLMChain(llm=llm, prompt=prompt_template3,memory=memory3)\n",
        "\n",
        "# User input and responses\n",
        "while(True):\n",
        "  question = input(\"Enter exit to exit: \")\n",
        "\n",
        "  if(question.lower()==\"exit\"):\n",
        "    break\n",
        "  else:\n",
        "    question = input(\"Enter question: \")\n",
        "    response = chain1.run({\"question\": question})\n",
        "    print(\"Answer:\", response)\n",
        "\n",
        "    question = input(\"Fact check chatbot enter query to check the fact: \")\n",
        "    response = chain2.run({\"question\": question})\n",
        "    print(\"Answer:\", response)\n",
        "\n",
        "    question = input(\"Personal AI assistant: \")\n",
        "    response = chain3.run({\"question\": question})\n",
        "    print(\"Answer:\", response)\n"
      ],
      "metadata": {
        "id": "ukrcmp2D7wDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b23080-b93f-4787-c154-48eb9cbf3201"
      },
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter exit to exit: c\n",
            "Enter question: hi\n",
            "Answer: Hello.\n",
            "Fact check chatbot enter query to check the fact: founder of openai\n",
            "Answer: Sam Altman\n",
            "Personal AI assistant: who are you\n",
            "Answer: I am jarvis, a helpful and knowledgeable AI assistant.  I'm here to answer your questions to the best of my ability.\n",
            "Enter exit to exit: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkjxEwyk-Lyz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiA5pbnj-4wv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade langchain-text-splitters langchain-community langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eErQBmT5Fxg",
        "outputId": "64320ad8-da2b-4dd7-becf-1346673c0a79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/2.5 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    }
  ]
}